{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e2d30e",
   "metadata": {},
   "source": [
    "# Self-attention execution with debugging print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc66040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing self-attention with debug prints...\n",
      "\n",
      "==================================================\n",
      "SELF-ATTENTION FORWARD PASS DEBUG\n",
      "==================================================\n",
      "\n",
      "Input word encodings shape: torch.Size([3, 2])\n",
      "Input word encodings:\n",
      "tensor([[ 1.1600,  0.2600],\n",
      "        [ 0.5700,  1.3600],\n",
      "        [ 4.4100, -2.1600]])\n",
      "\n",
      "--- Weight Matrices ---\n",
      "W_q weight matrix shape: torch.Size([2, 2])\n",
      "W_q weight matrix:\n",
      "Parameter containing:\n",
      "tensor([[ 0.5406,  0.5869],\n",
      "        [-0.1657,  0.6496]], requires_grad=True)\n",
      "\n",
      "W_k weight matrix shape: torch.Size([2, 2])\n",
      "W_k weight matrix:\n",
      "Parameter containing:\n",
      "tensor([[-0.1549,  0.1427],\n",
      "        [-0.3443,  0.4153]], requires_grad=True)\n",
      "\n",
      "W_v weight matrix shape: torch.Size([2, 2])\n",
      "W_v weight matrix:\n",
      "Parameter containing:\n",
      "tensor([[ 0.6233, -0.5188],\n",
      "        [ 0.6146,  0.1323]], requires_grad=True)\n",
      "\n",
      "--- Q, K, V Values ---\n",
      "Q (queries) shape: torch.Size([3, 2])\n",
      "Q (queries):\n",
      "tensor([[ 0.7797, -0.0233],\n",
      "        [ 1.1063,  0.7890],\n",
      "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)\n",
      "\n",
      "K (keys) shape: torch.Size([3, 2])\n",
      "K (keys):\n",
      "tensor([[-0.1426, -0.2914],\n",
      "        [ 0.1057,  0.3685],\n",
      "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)\n",
      "\n",
      "V (values) shape: torch.Size([3, 2])\n",
      "V (values):\n",
      "tensor([[ 0.5882,  0.7474],\n",
      "        [-0.3502,  0.5303],\n",
      "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)\n",
      "\n",
      "--- Attention Computation ---\n",
      "Unscaled attention scores (Q·K^T) shape: torch.Size([3, 3])\n",
      "Unscaled attention scores (Q·K^T):\n",
      "tensor([[-0.1044,  0.0739, -0.7168],\n",
      "        [-0.3877,  0.4078, -3.0024],\n",
      "        [ 0.4624, -0.6683,  4.0461]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Scale factor (sqrt(d_k)): 1.4142135381698608\n",
      "Scaled attention scores shape: torch.Size([3, 3])\n",
      "Scaled attention scores:\n",
      "tensor([[-0.0738,  0.0522, -0.5069],\n",
      "        [-0.2741,  0.2883, -2.1230],\n",
      "        [ 0.3270, -0.4725,  2.8610]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Attention probabilities (after softmax) shape: torch.Size([3, 3])\n",
      "Attention probabilities:\n",
      "tensor([[0.3593, 0.4076, 0.2330],\n",
      "        [0.3434, 0.6026, 0.0540],\n",
      "        [0.0712, 0.0320, 0.8969]], grad_fn=<SoftmaxBackward0>)\n",
      "Row sums (should be ~1.0): tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "Final attention output shape: torch.Size([3, 2])\n",
      "Final attention output:\n",
      "tensor([[0.9704, 1.0498],\n",
      "        [0.2001, 0.7072],\n",
      "        [3.5010, 2.2447]], grad_fn=<MmBackward0>)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9704, 1.0498],\n",
       "        [0.2001, 0.7072],\n",
       "        [3.5010, 2.2447]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from self_attention import selfAttention,execute_self_attention\n",
    "\n",
    "\n",
    "token_encodings = torch.tensor([[1.16,0.26],\n",
    "              [0.57,1.36],\n",
    "              [4.41,-2.16]])\n",
    "\n",
    "execute_self_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0b129",
   "metadata": {},
   "source": [
    "# Masked self attention with mask parameter being False\n",
    "\n",
    "### This should match the result with the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aefdc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing self-attention with debug prints...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9704, 1.0498],\n",
       "        [0.2001, 0.7072],\n",
       "        [3.5010, 2.2447]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from masked_self_attention import execute_masked_self_attention\n",
    "\n",
    "execute_masked_self_attention(mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1ab74",
   "metadata": {},
   "source": [
    "# Masked self attention with mask parameter being True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6993bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing self-attention with debug prints...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5882,  0.7474],\n",
       "        [-0.0096,  0.6091],\n",
       "        [ 3.5010,  2.2447]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from masked_self_attention import execute_masked_self_attention\n",
    "\n",
    "execute_masked_self_attention(mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e31ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
